#!/bin/bash

echo "ğŸš€ DÃ©marrage de la pipeline TechFeed Kafka..."

# CrÃ©er les rÃ©pertoires nÃ©cessaires
mkdir -p hadoop/config spark/apps spark/data logs

# Corriger les fichiers XML Hadoop si nÃ©cessaire
if [ -f "hadoop/config/core-site.xml" ]; then
    sed -i.bak 's/<n>/<name>/g' hadoop/config/core-site.xml
    sed -i.bak 's/<\/n>/<\/name>/g' hadoop/config/core-site.xml
fi

if [ -f "hadoop/config/hdfs-site.xml" ]; then
    sed -i.bak 's/<n>/<name>/g' hadoop/config/hdfs-site.xml  
    sed -i.bak 's/<\/n>/<\/name>/g' hadoop/config/hdfs-site.xml
fi

echo "ğŸ“¦ DÃ©marrage des services..."

# ArrÃªter les services existants
docker-compose down --remove-orphans 2>/dev/null || true

# DÃ©marrer tous les services
docker-compose up -d

echo "â³ Attente du dÃ©marrage des services..."
sleep 60

echo "âœ… Pipeline dÃ©marrÃ©e!"
echo ""
echo "ğŸŒ Interfaces disponibles:"
echo "  â€¢ Kafka UI:           http://localhost:8080"
echo "  â€¢ Spark Master UI:    http://localhost:8081"
echo "  â€¢ Hadoop NameNode UI: http://localhost:9870"
echo ""
echo "ğŸ“Š VÃ©rifier le statut: docker-compose ps"
echo "ğŸ›‘ ArrÃªter la pipeline: docker-compose down" 