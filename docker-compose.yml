version: '3.8'

services:
  # ==================== ZOOKEEPER ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: techfeed-zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - techfeed-network

  # ==================== KAFKA ====================
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: techfeed-kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - techfeed-network

  # ==================== KAFKA UI ====================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: techfeed-kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: techfeed-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - techfeed-network

  # ==================== HADOOP NAMENODE ====================
  namenode:
    image: apache/hadoop:3.3.4
    hostname: namenode
    container_name: techfeed-namenode
    command: ["hdfs", "namenode"]
    ports:
      - "9870:9870"
      - "8020:8020"
    env_file:
      - ./hadoop/hadoop.env
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      - namenode_data:/tmp/hadoop-root/dfs/name
      - ./hadoop/config:/opt/hadoop/etc/hadoop
    networks:
      - techfeed-network

  # ==================== HADOOP DATANODE ====================
  datanode:
    image: apache/hadoop:3.3.4
    hostname: datanode
    container_name: techfeed-datanode
    command: ["hdfs", "datanode"]
    ports:
      - "9864:9864"
    env_file:
      - ./hadoop/hadoop.env
    volumes:
      - datanode_data:/tmp/hadoop-root/dfs/data
      - ./hadoop/config:/opt/hadoop/etc/hadoop
    depends_on:
      - namenode
    networks:
      - techfeed-network

  # ==================== HADOOP RESOURCE MANAGER ====================
  resourcemanager:
    image: apache/hadoop:3.3.4
    hostname: resourcemanager
    container_name: techfeed-resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - "8088:8088"
    env_file:
      - ./hadoop/hadoop.env
    volumes:
      - ./hadoop/config:/opt/hadoop/etc/hadoop
    depends_on:
      - namenode
      - datanode
    networks:
      - techfeed-network

  # ==================== HADOOP NODE MANAGER ====================
  nodemanager:
    image: apache/hadoop:3.3.4
    hostname: nodemanager
    container_name: techfeed-nodemanager
    command: ["yarn", "nodemanager"]
    ports:
      - "8042:8042"
    env_file:
      - ./hadoop/hadoop.env
    volumes:
      - ./hadoop/config:/opt/hadoop/etc/hadoop
    depends_on:
      - namenode
      - datanode
      - resourcemanager
    networks:
      - techfeed-network

  # ==================== SPARK MASTER ====================
  spark-master:
    image: bitnami/spark:3.4.1
    hostname: spark-master
    container_name: techfeed-spark-master
    ports:
      - "8081:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - techfeed-network

  # ==================== SPARK WORKER 1 ====================
  spark-worker-1:
    image: bitnami/spark:3.4.1
    hostname: spark-worker-1
    container_name: techfeed-spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8082:8080"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - techfeed-network

  # ==================== SPARK WORKER 2 ====================
  spark-worker-2:
    image: bitnami/spark:3.4.1
    hostname: spark-worker-2
    container_name: techfeed-spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "8083:8080"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - techfeed-network

  # ==================== KAFKA TOPICS SETUP ====================
  kafka-setup:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka-setup
    container_name: techfeed-kafka-setup
    depends_on:
      - kafka
    command: |
      bash -c '
        echo "Attente de Kafka..."
        sleep 30
        
        echo "Création des topics Kafka..."
        
        # Topic pour les interactions utilisateur
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic user-interactions
        
        # Topic pour les vues de contenu
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic content-views
        
        # Topic pour les likes/dislikes
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic content-reactions
        
        # Topic pour les nouveaux contenus
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 2 --replication-factor 1 --topic new-content
        
        # Topic pour les utilisateurs (signups, updates)
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 2 --replication-factor 1 --topic user-events
        
        # Topic pour les recommandations calculées
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 2 --replication-factor 1 --topic recommendations
        
        # Topic pour les métriques et analytics
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 3 --replication-factor 1 --topic analytics-metrics
        
        # Topic pour les recherches
        kafka-topics --create --if-not-exists --bootstrap-server kafka:29092 --partitions 2 --replication-factor 1 --topic search-events
        
        echo "Topics créés avec succès!"
        kafka-topics --list --bootstrap-server kafka:29092
      '
    networks:
      - techfeed-network

  # ==================== SPARK STREAMING JOB ====================
  spark-streaming:
    image: bitnami/spark:3.4.1
    hostname: spark-streaming
    container_name: techfeed-spark-streaming
    depends_on:
      - spark-master
      - kafka
      - namenode
    command: |
      bash -c '
        echo "Attente des services..."
        sleep 60
        echo "Démarrage du job Spark Streaming..."
        spark-submit \
          --master spark://spark-master:7077 \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.apache.hadoop:hadoop-client:3.3.4 \
          --conf spark.sql.adaptive.enabled=true \
          --conf spark.sql.adaptive.coalescePartitions.enabled=true \
          /opt/spark-apps/streaming_job.py
      '
    volumes:
      - ./spark/apps:/opt/spark-apps
      - ./spark/data:/opt/spark-data
    networks:
      - techfeed-network

  # ==================== POSTGRES (pour comparaison/backup) ====================
  postgres:
    image: postgres:15
    container_name: techfeed-postgres
    environment:
      POSTGRES_DB: techfeed
      POSTGRES_USER: tompicout
      POSTGRES_PASSWORD: password
    ports:
      - "5433:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - techfeed-network

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  namenode_data:
  datanode_data:
  postgres_data:

networks:
  techfeed-network:
    driver: bridge 